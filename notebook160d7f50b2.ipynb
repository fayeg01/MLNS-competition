{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Setup"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-10T09:40:00.762388Z","iopub.status.busy":"2022-03-10T09:40:00.761938Z","iopub.status.idle":"2022-03-10T09:40:02.556970Z","shell.execute_reply":"2022-03-10T09:40:02.555288Z","shell.execute_reply.started":"2022-03-10T09:40:00.762295Z"},"trusted":true},"outputs":[],"source":["import random\n","import numpy as np\n","import igraph\n","from sklearn import svm\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import linear_kernel\n","from sklearn import preprocessing\n","import nltk\n","import csv\n","import networkx as nx\n","import tqdm\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-03-10T09:46:59.475516Z","iopub.status.busy":"2022-03-10T09:46:59.475194Z","iopub.status.idle":"2022-03-10T09:46:59.652937Z","shell.execute_reply":"2022-03-10T09:46:59.652025Z","shell.execute_reply.started":"2022-03-10T09:46:59.475483Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/geraud/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /home/geraud/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('punkt') # for tokenization\n","nltk.download('stopwords')\n","stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n","stemmer = nltk.stem.PorterStemmer()\n","\n","with open(\"testing_set.txt\", \"r\") as f:\n","    reader = csv.reader(f)\n","    testing_set  = list(reader)\n","\n","testing_set = [element[0].split(\" \") for element in testing_set]"]},{"cell_type":"markdown","metadata":{},"source":["Please choose if you want to generate features or use the already generated ones (it takes some hours to recreate them).\n","\n","You can also choose the model you want to test."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Recreation of features\n","recreate_training_features = False\n","recreate_testing_features = False\n","recreate_graphs = False\n","# Saving recreated features\n","save_recreated_features = False\n","# Model to use\n","allowed_algorithms = ['SVM', 'RandomForest', 'XGBoost']\n","algorithm = 'XGBoost'\n","if algorithm not in allowed_algorithms:\n","    algorithm = 'XGBoost'"]},{"cell_type":"markdown","metadata":{},"source":["# Creating training features"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-03-10T09:48:27.745085Z","iopub.status.busy":"2022-03-10T09:48:27.744629Z","iopub.status.idle":"2022-03-10T09:48:29.774751Z","shell.execute_reply":"2022-03-10T09:48:29.773794Z","shell.execute_reply.started":"2022-03-10T09:48:27.745031Z"},"trusted":true},"outputs":[],"source":["with open(\"training_set.txt\", \"r\") as f:\n","    reader = csv.reader(f)\n","    training_set  = list(reader)\n","\n","training_set = [element[0].split(\" \") for element in training_set]\n","\n","with open(\"node_information.csv\", \"r\", newline='\\n') as f:\n","    reader = csv.reader(f, delimiter=',')\n","    node_info  = list(reader)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-03-10T09:48:33.581369Z","iopub.status.busy":"2022-03-10T09:48:33.580503Z","iopub.status.idle":"2022-03-10T09:48:35.498815Z","shell.execute_reply":"2022-03-10T09:48:35.498120Z","shell.execute_reply.started":"2022-03-10T09:48:33.581322Z"},"trusted":true},"outputs":[],"source":["IDs = [element[0] for element in node_info]\n","# compute TFIDF vector of each paper\n","corpus = [element[5] for element in node_info]\n","vectorizer = TfidfVectorizer(stop_words=\"english\")\n","# each row is a node in the order of node_info\n","features_TFIDF = vectorizer.fit_transform(corpus)"]},{"cell_type":"markdown","metadata":{},"source":["## Creating graph features between publications"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["if recreate_graphs:\n","    G_article = nx.Graph()\n","    article_list = IDs\n","    for article in article_list:\n","        G_article.add_node(article)\n","    pbar = tqdm.tqdm(total=len(training_set))\n","    for edge in training_set:\n","        if edge[2] == '0':\n","            pbar.update(1)\n","            continue\n","        else:\n","            art1 = int(edge[0])\n","            art2 = int(edge[1])\n","            G_article.add_edge(art1, art2)\n","        pbar.update(1)\n","    pbar.close()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["if not recreate_graphs:\n","    G_article = nx.read_graphml(\"G_article.graphml\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def feature_extractor_article(graph, samples):\n","    feature_vector = [[],[],[],[],[]]\n","    deg_centrality = nx.degree_centrality(graph)\n","    pbar = tqdm.tqdm(total=len(samples))\n","    for edge in samples:\n","        source_node, target_node = edge[0], edge[1]\n","        source_degree_centrality = deg_centrality[source_node]\n","        target_degree_centrality = deg_centrality[target_node]\n","        pref_attach = list(nx.preferential_attachment(graph, [(source_node, target_node)]))[0][2]\n","        if source_node == target_node:\n","            aai = 1\n","        else:\n","            aai = list(nx.adamic_adar_index(graph, [(source_node, target_node)]))[0][2]\n","        jacard_coeff = list(nx.jaccard_coefficient(graph, [(source_node, target_node)]))[0][2]\n","        feature_vector[0].append(source_degree_centrality)\n","        feature_vector[1].append(target_degree_centrality)\n","        feature_vector[2].append(pref_attach)\n","        feature_vector[3].append(aai)\n","        feature_vector[4].append(jacard_coeff)\n","        pbar.update(1)\n","    pbar.close()\n","    return feature_vector\n","\n","if recreate_training_features:\n","    graph_features_article = feature_extractor_article(G_article, training_set)\n","    source_degree_centrality_article = graph_features_article[0]\n","    target_degree_centrality_article = graph_features_article[1]\n","    pref_attach_article = graph_features_article[2]\n","    aai_article = graph_features_article[3]\n","    jacard_coeff_article = graph_features_article[4]\n","    if save_recreated_features:\n","        np.save('source_degree_centrality_article', np.array(graph_features_article[0]))\n","        np.save('target_degree_centrality_article', np.array(graph_features_article[1]))\n","        np.save('pref_attach_article', np.array(graph_features_article[2]))\n","        np.save('aai_article', np.array(graph_features_article[3]))\n","        np.save('jacard_coeff_article', np.array(graph_features_article[4]))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-03-10T09:49:47.369766Z","iopub.status.busy":"2022-03-10T09:49:47.369463Z","iopub.status.idle":"2022-03-10T10:05:51.142279Z","shell.execute_reply":"2022-03-10T10:05:51.140723Z","shell.execute_reply.started":"2022-03-10T09:49:47.369732Z"},"trusted":true},"outputs":[],"source":["if recreate_graphs:\n","    G_journal = nx.Graph()\n","    journal_list = list(set([element[4] for element in node_info]))\n","    for journal in journal_list:\n","        G_journal.add_node(journal)\n","    pbar = tqdm.tqdm(total=len(training_set))\n","    for edge in training_set:\n","        if edge[2] == '0':\n","            pbar.update(1)\n","            continue\n","        else:\n","            journ1 = 0\n","            journ2 = 0\n","            for i in range(len(IDs)):\n","                if int(IDs[i]) == int(edge[0]):\n","                    journ1 = i\n","                if int(IDs[i]) == int(edge[1]):\n","                    journ2 = i\n","            G_journal.add_edge(node_info[journ1][4], node_info[journ2][4])\n","        pbar.update(1)\n","    pbar.close()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["if not recreate_graphs:\n","    G_journal = nx.read_graphml(\"G_journal.graphml\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-03-10T10:21:18.979192Z","iopub.status.busy":"2022-03-10T10:21:18.978424Z","iopub.status.idle":"2022-03-10T10:46:37.035488Z","shell.execute_reply":"2022-03-10T10:46:37.034480Z","shell.execute_reply.started":"2022-03-10T10:21:18.979146Z"},"trusted":true},"outputs":[],"source":["def feature_extractor_journal(graph, samples):\n","    feature_vector = [[],[],[],[],[]]\n","    deg_centrality = nx.degree_centrality(graph)\n","    pbar = tqdm.tqdm(total=len(samples))\n","    for edge in samples:\n","        journ1 = 0\n","        journ2 = 0\n","        for i in range(len(IDs)):\n","            if int(IDs[i]) == int(edge[0]):\n","                journ1 = i\n","            if int(IDs[i]) == int(edge[1]):\n","                journ2 = i\n","        source_node, target_node = node_info[journ1][4], node_info[journ2][4]\n","        source_degree_centrality = deg_centrality[source_node]\n","        target_degree_centrality = deg_centrality[target_node]\n","        pref_attach = list(nx.preferential_attachment(graph, [(source_node, target_node)]))[0][2]\n","        if source_node == target_node:\n","            aai = 1\n","        else:\n","            aai = list(nx.adamic_adar_index(graph, [(source_node, target_node)]))[0][2]\n","        jacard_coeff = list(nx.jaccard_coefficient(graph, [(source_node, target_node)]))[0][2]\n","        feature_vector[0].append(source_degree_centrality)\n","        feature_vector[1].append(target_degree_centrality)\n","        feature_vector[2].append(pref_attach)\n","        feature_vector[3].append(aai)\n","        feature_vector[4].append(jacard_coeff)\n","        pbar.update(1)\n","    pbar.close()\n","    return feature_vector\n","\n","if recreate_training_features:\n","    graph_features_journal = feature_extractor_journal(G_journal, training_set)\n","    source_degree_centrality_journal = graph_features_journal[0]\n","    target_degree_centrality_journal = graph_features_journal[1]\n","    pref_attach_journal = graph_features_journal[2]\n","    aai_journal = graph_features_journal[3]\n","    jacard_coeff_journal = graph_features_journal[4]\n","    if save_recreated_features:\n","        np.save('source_degree_centrality_journal', np.array(graph_features_journal[0]))\n","        np.save('target_degree_centrality_journal', np.array(graph_features_journal[1]))\n","        np.save('pref_attach_journal', np.array(graph_features_journal[2]))\n","        np.save('aai_journal', np.array(graph_features_journal[3]))\n","        np.save('jacard_coeff_journal', np.array(graph_features_journal[4]))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-03-10T10:57:11.385146Z","iopub.status.busy":"2022-03-10T10:57:11.384782Z","iopub.status.idle":"2022-03-10T11:14:19.384429Z","shell.execute_reply":"2022-03-10T11:14:19.383500Z","shell.execute_reply.started":"2022-03-10T10:57:11.385107Z"},"trusted":true},"outputs":[],"source":["if recreate_graphs:\n","    G_authors = nx.Graph()\n","    authors_list = []\n","    for element in node_info:\n","        to_append = element[3].split(sep=',')\n","        for x in to_append:\n","            authors_list.append(x)\n","    authors_list = list(set(authors_list))\n","    for author in authors_list:\n","        G_authors.add_node(author)\n","    pbar = tqdm.tqdm(total=len(training_set))\n","    for edge in training_set:\n","        if edge[2] == '0':\n","            pbar.update(1)\n","            continue\n","        else:\n","            authors1 = []\n","            authors2 = []\n","            for i in range(len(IDs)):\n","                if int(IDs[i]) == int(edge[0]):\n","                    authors1 = node_info[i][3].split(sep=',')\n","                if int(IDs[i]) == int(edge[1]):\n","                    authors2 = node_info[i][3].split(sep=',')\n","            for author1 in authors1:\n","                for author2 in authors2:\n","                    G_authors.add_edge(author1, author2)\n","        pbar.update(1)\n","    pbar.close()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["if not recreate_graphs:\n","    G_authors = nx.read_graphml(\"G_authors.graphml\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.status.busy":"2022-03-10T10:46:37.100938Z","iopub.status.idle":"2022-03-10T10:46:37.101267Z","shell.execute_reply":"2022-03-10T10:46:37.101131Z","shell.execute_reply.started":"2022-03-10T10:46:37.101114Z"},"trusted":true},"outputs":[],"source":["def feature_extractor_author(graph, samples):\n","    feature_vector = [[],[],[],[],[]]\n","    deg_centrality = nx.degree_centrality(graph)\n","    pbar = tqdm.tqdm(total=len(samples))\n","    for edge in samples:\n","        authors1 = []\n","        authors2 = []\n","        for i in range(len(IDs)):\n","            if int(IDs[i]) == int(edge[0]):\n","                authors1 = node_info[i][3]\n","            if int(IDs[i]) == int(edge[1]):\n","                authors2 = node_info[i][3]\n","        source_authors, target_authors = authors1.split(sep=','), authors2.split(sep=',')\n","        source_centralities = [deg_centrality[source_author] for source_author in source_authors]\n","        source_degree_centrality = np.amax(source_centralities)\n","        target_centralities = [deg_centrality[target_author] for target_author in target_authors]\n","        target_degree_centrality = np.amax(target_centralities)\n","        pref_attachs = [[list(nx.preferential_attachment(graph, [(source_author, target_author)]))[0][2] for source_author in source_authors] for target_author in target_authors]\n","        pref_attach = np.amax(pref_attachs)\n","        aais = []\n","        for source_author in source_authors:\n","            for target_author in target_authors:\n","                if target_author == source_author:\n","                    aais.append(1)\n","                else:\n","                    aais.append(list(nx.adamic_adar_index(graph, [(source_author, target_author)]))[0][2])\n","        aai = np.amax(aais)\n","        jacard_coeffs = [[list(nx.jaccard_coefficient(graph, [(source_author, target_author)]))[0][2] for source_author in source_authors] for target_author in target_authors]\n","        jacard_coeff = np.amax(jacard_coeffs)\n","        feature_vector[0].append(source_degree_centrality)\n","        feature_vector[1].append(target_degree_centrality)\n","        feature_vector[2].append(pref_attach)\n","        feature_vector[3].append(aai)\n","        feature_vector[4].append(jacard_coeff)\n","        pbar.update(1)\n","    pbar.close()\n","    return feature_vector\n","\n","def feature_extractor_author_mean(graph, samples):\n","    feature_vector = [[],[],[],[],[]]\n","    deg_centrality = nx.degree_centrality(graph)\n","    pbar = tqdm.tqdm(total=len(samples))\n","    for edge in samples:\n","        authors1 = []\n","        authors2 = []\n","        for i in range(len(IDs)):\n","            if int(IDs[i]) == int(edge[0]):\n","                authors1 = node_info[i][3]\n","            if int(IDs[i]) == int(edge[1]):\n","                authors2 = node_info[i][3]\n","        source_authors, target_authors = authors1.split(sep=','), authors2.split(sep=',')\n","        source_centralities = [deg_centrality[source_author] for source_author in source_authors]\n","        source_degree_centrality = np.mean(source_centralities)\n","        target_centralities = [deg_centrality[target_author] for target_author in target_authors]\n","        target_degree_centrality = np.mean(target_centralities)\n","        pref_attachs = [[list(nx.preferential_attachment(graph, [(source_author, target_author)]))[0][2] for source_author in source_authors] for target_author in target_authors]\n","        pref_attach = np.mean(pref_attachs)\n","        aais = []\n","        for source_author in source_authors:\n","            for target_author in target_authors:\n","                if target_author == source_author:\n","                    aais.append(1)\n","                else:\n","                    aais.append(list(nx.adamic_adar_index(graph, [(source_author, target_author)]))[0][2])\n","        aai = np.mean(aais)\n","        jacard_coeffs = [[list(nx.jaccard_coefficient(graph, [(source_author, target_author)]))[0][2] for source_author in source_authors] for target_author in target_authors]\n","        jacard_coeff = np.mean(jacard_coeffs)\n","        feature_vector[0].append(source_degree_centrality)\n","        feature_vector[1].append(target_degree_centrality)\n","        feature_vector[2].append(pref_attach)\n","        feature_vector[3].append(aai)\n","        feature_vector[4].append(jacard_coeff)\n","        pbar.update(1)\n","    pbar.close()\n","    return feature_vector\n","\n","if recreate_training_features:\n","    graph_features_author = feature_extractor_author(G_authors, training_set)\n","    source_degree_centrality_author = graph_features_author[0]\n","    target_degree_centrality_author = graph_features_author[1]\n","    pref_attach_author = graph_features_author[2]\n","    aai_author = graph_features_author[3]\n","    jacard_coeff_author = graph_features_author[4]\n","    graph_features_author = feature_extractor_author_mean(G_authors, training_set)\n","    source_degree_centrality_author_mean = graph_features_author[0]\n","    target_degree_centrality_author_mean = graph_features_author[1]\n","    pref_attach_author_mean = graph_features_author[2]\n","    aai_author_mean = graph_features_author[3]\n","    jacard_coeff_author_mean = graph_features_author[4]\n","    if save_recreated_features:\n","        np.save('source_degree_centrality_author', source_degree_centrality_author)\n","        np.save('target_degree_centrality_author', target_degree_centrality_author)\n","        np.save('pref_attach_author', pref_attach_author)\n","        np.save('aai_author', aai_author)\n","        np.save('jacard_coeff_author', jacard_coeff_author)\n","        np.save('source_degree_centrality_author_mean', source_degree_centrality_author_mean)\n","        np.save('target_degree_centrality_author_mean', target_degree_centrality_author_mean)\n","        np.save('pref_attach_author_mean', pref_attach_author_mean)\n","        np.save('aai_author_mean', aai_author_mean)\n","        np.save('jacard_coeff_author_mean', jacard_coeff_author_mean)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.status.busy":"2022-03-10T10:05:51.192081Z","iopub.status.idle":"2022-03-10T10:05:51.192429Z","shell.execute_reply":"2022-03-10T10:05:51.192275Z","shell.execute_reply.started":"2022-03-10T10:05:51.192251Z"},"trusted":true},"outputs":[],"source":["if recreate_training_features:\n","    # number of overlapping words in title\n","    overlap_title = []\n","    # temporal distance between the papers\n","    temp_diff = []\n","    # number of common authors\n","    comm_auth = []\n","    # same journal\n","    same_journal = []\n","    # number of overlapping words in abstract\n","    overlap_abstract = []\n","\n","    counter = 0\n","    print(len(training_set))\n","    for i in range(len(training_set)):\n","        source = training_set[i][0]\n","        target = training_set[i][1]\n","        \n","        index_source = IDs.index(source)\n","        index_target = IDs.index(target)\n","        \n","        source_info = [element for element in node_info if element[0]==source][0]\n","        target_info = [element for element in node_info if element[0]==target][0]\n","        \n","        # convert to lowercase and tokenize\n","        source_title = source_info[2].lower().split(\" \")\n","        # remove stopwords\n","        source_title = [token for token in source_title if token not in stpwds]\n","        source_title = [stemmer.stem(token) for token in source_title]\n","        \n","        target_title = target_info[2].lower().split(\" \")\n","        target_title = [token for token in target_title if token not in stpwds]\n","        target_title = [stemmer.stem(token) for token in target_title]\n","        \n","        source_auth = source_info[3].split(\",\")\n","        target_auth = target_info[3].split(\",\")\n","        \n","        id_journ = int(source_info[4] == target_info[4])\n","        \n","        # convert to lowercase and tokenize\n","        source_abstract = source_info[5].lower().split(\" \")\n","        # remove stopwords\n","        source_abstract = [token for token in source_title if token not in stpwds]\n","        source_abstract = [stemmer.stem(token) for token in source_title]\n","        \n","        target_abstract = target_info[5].lower().split(\" \")\n","        target_abstract = [token for token in target_title if token not in stpwds]\n","        target_abstract = [stemmer.stem(token) for token in target_title]\n","        \n","        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n","        temp_diff.append(int(source_info[1]) - int(target_info[1]))\n","        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n","        same_journal.append(id_journ)\n","        overlap_abstract.append(len(set(source_abstract).intersection(set(target_abstract))))\n","    \n","        counter += 1\n","        if counter % 10000 == True:\n","            print(counter, \"training examples processsed\\r\")\n","\n","    overlap_title_sqrt = np.sqrt(overlap_title)\n","    temp_diff_sqrt = np.sqrt(np.abs(temp_diff))\n","    overlap_abstract_sqrt = np.sqrt(overlap_abstract)\n","\n","    if save_recreated_features:\n","        np.save('overlap_title', np.array(overlap_title))\n","        np.save('temp_diff', np.array(temp_diff))\n","        np.save('comm_auth', np.array(comm_auth))\n","        np.save('same_journal', np.array(same_journal))\n","        np.save('overlap_abstract', np.array(overlap_abstract))\n","        np.save('overlap_title_sqrt', np.array(overlap_title_sqrt))\n","        np.save('temp_diff_sqrt', np.array(temp_diff_sqrt))\n","        np.save('overlap_abstract_sqrt', np.array(overlap_abstract_sqrt))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["if not recreate_training_features:\n","    source_degree_centrality_article = np.load('source_degree_centrality_article.npy')\n","    target_degree_centrality_article = np.load('target_degree_centrality_article.npy')\n","    pref_attach_article = np.load('pref_attach_article.npy')\n","    aai_article = np.load('aai_article.npy')\n","    jacard_coeff_article = np.load('jacard_coeff_article.npy')\n","    source_degree_centrality_journal = np.load('source_degree_centrality_journal.npy')\n","    target_degree_centrality_journal = np.load('target_degree_centrality_journal.npy')\n","    pref_attach_journal = np.load('pref_attach_journal.npy')\n","    aai_journal = np.load('aai_journal.npy')\n","    jacard_coeff_journal = np.load('jacard_coeff_journal.npy')\n","    source_degree_centrality_author = np.load('source_degree_centrality_author.npy')\n","    target_degree_centrality_author = np.load('target_degree_centrality_author.npy')\n","    pref_attach_author = np.load('pref_attach_author.npy')\n","    aai_author = np.load('aai_author.npy')\n","    jacard_coeff_author = np.load('jacard_coeff_author.npy')\n","    source_degree_centrality_author_mean = np.load('source_degree_centrality_author_mean.npy')\n","    target_degree_centrality_author_mean = np.load('target_degree_centrality_author_mean.npy')\n","    pref_attach_author_mean = np.load('pref_attach_author_mean.npy')\n","    aai_author_mean = np.load('aai_author_mean.npy')\n","    jacard_coeff_author_mean = np.load('jacard_coeff_author_mean.npy')\n","    overlap_title = np.load('overlap_title.npy')\n","    overlap_abstract = np.load('overlap_abstract.npy')\n","    temp_diff = np.load('temp_diff.npy')\n","    comm_auth = np.load('comm_auth.npy')\n","    same_journal = np.load('same_journal.npy')\n","    overlap_title_sqrt = np.load('overlap_title_sqrt.npy')\n","    temp_diff_sqrt = np.load('temp_diff_sqrt.npy')\n","    overlap_abstract_sqrt = np.load('overlap_abstract_sqrt.npy')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.status.busy":"2022-03-10T10:05:51.194151Z","iopub.status.idle":"2022-03-10T10:05:51.194540Z","shell.execute_reply":"2022-03-10T10:05:51.194387Z","shell.execute_reply.started":"2022-03-10T10:05:51.194360Z"},"trusted":true},"outputs":[],"source":["# convert list of lists into array\n","# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n","training_features = np.array([source_degree_centrality_article, target_degree_centrality_article, pref_attach_article, aai_article, jacard_coeff_article, overlap_title, temp_diff, comm_auth, same_journal, overlap_abstract, overlap_title_sqrt, temp_diff_sqrt, overlap_abstract_sqrt, source_degree_centrality_journal, target_degree_centrality_journal, pref_attach_journal, aai_journal, jacard_coeff_journal, source_degree_centrality_author, target_degree_centrality_author, pref_attach_author, aai_author, jacard_coeff_author, source_degree_centrality_author_mean, target_degree_centrality_author_mean, pref_attach_author_mean, aai_author_mean, jacard_coeff_author_mean]).T"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-03-09T18:40:41.922973Z","iopub.status.busy":"2022-03-09T18:40:41.922696Z","iopub.status.idle":"2022-03-09T18:40:44.605695Z","shell.execute_reply":"2022-03-09T18:40:44.604805Z","shell.execute_reply.started":"2022-03-09T18:40:41.922942Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","poly = PolynomialFeatures(2)\n","training_features = poly.fit_transform(training_features)\n","\n","# scale\n","scaler = preprocessing.StandardScaler().fit(training_features)\n","training_features = scaler.transform(training_features)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-03-09T18:40:44.608902Z","iopub.status.busy":"2022-03-09T18:40:44.60854Z","iopub.status.idle":"2022-03-09T18:40:44.614149Z","shell.execute_reply":"2022-03-09T18:40:44.613149Z","shell.execute_reply.started":"2022-03-09T18:40:44.608859Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(615512, 435)\n"]}],"source":["print(training_features.shape)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-03-09T18:40:44.615907Z","iopub.status.busy":"2022-03-09T18:40:44.615567Z","iopub.status.idle":"2022-03-09T18:40:45.118407Z","shell.execute_reply":"2022-03-09T18:40:45.117772Z","shell.execute_reply.started":"2022-03-09T18:40:44.615874Z"},"trusted":true},"outputs":[],"source":["# convert labels into integers then into column array\n","# labels = [int(element[2]) for element in training_set_reduced]\n","labels = [int(element[2]) for element in training_set]\n","labels = list(labels)\n","labels_array = np.array(labels)"]},{"cell_type":"markdown","metadata":{},"source":["# Using neural network for inference with 2 classes¶"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(435, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 2),\n","        )\n","        \n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","    \n","    def predict(self, x):\n","        return self.forward(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test(model, criterion, testloader, epoch):\n","    best_acc = 0\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","    print(f\"Epoch {epoch+1}\\t\\t Test accuracy:{correct/total}\")\n","    return correct/total\n","\n","def test(model, criterion, testloader, epoch):\n","    best_acc = 0\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","    print(f\"Epoch {epoch+1}\\t\\t Test accuracy:{correct/total}\")\n","    return correct/total\n","    \n","def train(model, optimizer, criterion, train_loader, testloader, epoch, log_step):\n","    model.train()\n","    correct = 0\n","    total = 0\n","    hist_test = [0, 0]\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","    if (epoch+1) % log_step == 0:\n","        print(f\"Epoch {epoch+1}\\t\\t Training accuracy:{correct/total}\")\n","        test_acc = test(model, criterion, testloader, epoch)\n","        hist_test.append(test_acc)\n","        if hist_test[-3] > hist_test[-1] and hist_test[-2] > hist_test[-1]:\n","            print(\"Early stopping\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier = NeuralNetwork().to(device)\n","tensor_features = torch.Tensor(training_features).to(device)\n","tensor_y = torch.tensor(labels_array, dtype=torch.long).to(device)\n","dataset_train = torch.utils.data.TensorDataset(tensor_features[:50000],tensor_y[:50000])\n","dataset_val = torch.utils.data.TensorDataset(tensor_features[50000:],tensor_y[50000:])\n","train_data_loader = torch.utils.data.DataLoader(dataset_train,\n","                                          batch_size=100,\n","                                          shuffle=True)\n","test_data_loader = torch.utils.data.DataLoader(dataset_val,\n","                                          batch_size=100,\n","                                          shuffle=True)\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(classifier.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(100):\n","    train(classifier, optimizer, criterion, train_data_loader, test_data_loader, epoch, 1)"]},{"cell_type":"markdown","metadata":{},"source":["# SVM"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["if algorithm == \"SVM\":\n","    # initialize basic SVM\n","    classifier = svm.LinearSVC(max_iter=20000)\n","    # train\n","    classifier.fit(training_features, labels_array)"]},{"cell_type":"markdown","metadata":{},"source":["# Random forest"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-02-28T08:52:35.487727Z","iopub.status.busy":"2022-02-28T08:52:35.487164Z","iopub.status.idle":"2022-02-28T08:59:14.606313Z","shell.execute_reply":"2022-02-28T08:59:14.605657Z","shell.execute_reply.started":"2022-02-28T08:52:35.487684Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","if algorithm == \"RandomForest\":\n","    classifier = RandomForestClassifier(n_estimators=200, n_jobs=-1)\n","    classifier.fit(training_features, labels_array)"]},{"cell_type":"markdown","metadata":{},"source":["# XGBoost"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-03-09T18:40:45.120024Z","iopub.status.busy":"2022-03-09T18:40:45.119775Z","iopub.status.idle":"2022-03-09T19:06:36.228795Z","shell.execute_reply":"2022-03-09T19:06:36.227388Z","shell.execute_reply.started":"2022-03-09T18:40:45.119994Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/geraud/Machine-Learning-in-Network-Science/.venv/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n","  from pandas import MultiIndex, Int64Index\n","/home/geraud/Machine-Learning-in-Network-Science/.venv/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]}],"source":["import xgboost as xgb\n","from sklearn.metrics import mean_squared_error\n","import pandas as pd\n","import numpy as np\n","\n","if algorithm == \"XGBoost\":\n","    data_dmatrix = xgb.DMatrix(data=training_features,label=labels_array)\n","\n","    classifier = xgb.XGBClassifier(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n","                    max_depth = 8, alpha = 10, n_estimators = 75)#, tree_method='gpu_hist')\n","\n","    classifier.fit(training_features, labels_array)"]},{"cell_type":"markdown","metadata":{},"source":["# Predictions on test set"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 32648/32648 [00:05<00:00, 5641.82it/s]\n"]}],"source":["if recreate_testing_features:\n","    graph_features_article = feature_extractor_article(G_article, testing_set)\n","    source_degree_centrality_article = graph_features_article[0]\n","    target_degree_centrality_article = graph_features_article[1]\n","    pref_attach_article = graph_features_article[2]\n","    aai_article = graph_features_article[3]\n","    jacard_coeff_article = graph_features_article[4]\n","    if save_recreated_features:\n","        np.save('source_degree_centrality_article', np.array(source_degree_centrality_article))\n","        np.save('target_degree_centrality_article', np.array(target_degree_centrality_article))\n","        np.save('pref_attach_article', np.array(pref_attach_article))\n","        np.save('aai_article', np.array(aai_article))\n","        np.save('jacard_coeff_article', np.array(jacard_coeff_article))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["if not recreate_testing_features:\n","    source_degree_centrality_article = np.load(\"source_degree_centrality_article.npy\")\n","    target_degree_centrality_article = np.load(\"target_degree_centrality_article.npy\")\n","    pref_attach_article = np.load(\"pref_attach_article.npy\")\n","    aai_article = np.load(\"aai_article.npy\")\n","    jacard_coeff_article = np.load(\"jacard_coeff_article.npy\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 32648/32648 [05:00<00:00, 108.57it/s]\n"]}],"source":["if recreate_testing_features:\n","    graph_features_journal = feature_extractor_journal(G_journal, testing_set)\n","    source_degree_centrality_journal = graph_features_journal[0]\n","    target_degree_centrality_journal = graph_features_journal[1]\n","    pref_attach_journal = graph_features_journal[2]\n","    aai_journal = graph_features_journal[3]\n","    jacard_coeff_journal = graph_features_journal[4]\n","    if save_recreated_features:\n","        np.save('source_degree_centrality_test_journal', np.array(source_degree_centrality_journal))\n","        np.save('target_degree_centrality_test_journal', np.array(target_degree_centrality_journal))\n","        np.save('pref_attach_test_journal', np.array(pref_attach_journal))\n","        np.save('aai_test_journal', np.array(aai_journal))\n","        np.save('jacard_coeff_test_journal', np.array(jacard_coeff_journal))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["if not recreate_testing_features:\n","    source_degree_centrality_journal = np.load(\"source_degree_centrality_test_journal.npy\")\n","    target_degree_centrality_journal = np.load(\"target_degree_centrality_test_journal.npy\")\n","    pref_attach_journal = np.load(\"pref_attach_test_journal.npy\")\n","    aai_journal = np.load(\"aai_test_journal.npy\")\n","    jacard_coeff_journal = np.load(\"jacard_coeff_test_journal.npy\")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["if recreate_testing_features:\n","    graph_features_author = feature_extractor_author(G_authors, testing_set)\n","    source_degree_centrality_author = graph_features_author[0]\n","    target_degree_centrality_author = graph_features_author[1]\n","    pref_attach_author = graph_features_author[2]\n","    aai_author = graph_features_author[3]\n","    jacard_coeff_author = graph_features_author[4]\n","    if save_recreated_features:\n","        np.save('source_degree_centrality_test_author', np.array(source_degree_centrality_author))\n","        np.save('target_degree_centrality_test_author', np.array(target_degree_centrality_author))\n","        np.save('pref_attach_test_author', np.array(pref_attach_author))\n","        np.save('aai_test_author', np.array(aai_author))\n","        np.save('jacard_coeff_test_author', np.array(jacard_coeff_author))\n","\n","    graph_features_author_mean = feature_extractor_author_mean(G_authors, testing_set)\n","    source_degree_centrality_author_mean = graph_features_author_mean[0]\n","    target_degree_centrality_author_mean = graph_features_author_mean[1]\n","    pref_attach_author_mean = graph_features_author_mean[2]\n","    aai_author_mean = graph_features_author_mean[3]\n","    jacard_coeff_author_mean = graph_features_author_mean[4]\n","    if save_recreated_features:\n","        np.save('source_degree_centrality_test_author_mean', np.array(source_degree_centrality_author_mean))\n","        np.save('target_degree_centrality_test_author_mean', np.array(target_degree_centrality_author_mean))\n","        np.save('pref_attach_test_author_mean', np.array(pref_attach_author_mean))\n","        np.save('aai_test_author_mean', np.array(aai_author_mean))\n","        np.save('jacard_coeff_test_author_mean', np.array(jacard_coeff_author_mean))"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["if not recreate_testing_features:\n","    source_degree_centrality_author = np.load(\"source_degree_centrality_test_author.npy\")\n","    target_degree_centrality_author = np.load(\"target_degree_centrality_test_author.npy\")\n","    pref_attach_author = np.load(\"pref_attach_test_author.npy\")\n","    aai_author = np.load(\"aai_test_author.npy\")\n","    jacard_coeff_author = np.load(\"jacard_coeff_test_author.npy\")\n","\n","    source_degree_centrality_author_mean = np.load(\"source_degree_centrality_test_author_mean.npy\")\n","    target_degree_centrality_author_mean = np.load(\"target_degree_centrality_test_author_mean.npy\")\n","    pref_attach_author_mean = np.load(\"pref_attach_test_author_mean.npy\")\n","    aai_author_mean = np.load(\"aai_test_author_mean.npy\")\n","    jacard_coeff_author_mean = np.load(\"jacard_coeff_test_author_mean.npy\")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-03-09T19:08:10.43374Z","iopub.status.busy":"2022-03-09T19:08:10.433431Z","iopub.status.idle":"2022-03-09T19:11:06.524714Z","shell.execute_reply":"2022-03-09T19:11:06.523757Z","shell.execute_reply.started":"2022-03-09T19:08:10.433686Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1 testing examples processsed\n","10001 testing examples processsed\n","20001 testing examples processsed\n","30001 testing examples processsed\n"]}],"source":["# These features are quite fast to recreate, we didn't save them\n","\n","overlap_title_test = []\n","temp_diff_test = []\n","comm_auth_test = []\n","same_journal_test = []\n","overlap_abstract_test = []\n","    \n","counter = 0\n","for i in range(len(testing_set)):\n","    source = testing_set[i][0]\n","    target = testing_set[i][1]\n","    \n","    index_source = IDs.index(source)\n","    index_target = IDs.index(target)\n","    \n","    source_info = [element for element in node_info if element[0]==source][0]\n","    target_info = [element for element in node_info if element[0]==target][0]\n","    \n","    source_title = source_info[2].lower().split(\" \")\n","    source_title = [token for token in source_title if token not in stpwds]\n","    source_title = [stemmer.stem(token) for token in source_title]\n","    \n","    target_title = target_info[2].lower().split(\" \")\n","    target_title = [token for token in target_title if token not in stpwds]\n","    target_title = [stemmer.stem(token) for token in target_title]\n","    \n","    source_auth = source_info[3].split(\",\")\n","    target_auth = target_info[3].split(\",\")\n","    \n","    id_journ = int(source_info[4] == target_info[4])\n","    \n","\t# convert to lowercase and tokenize\n","    source_abstract = source_info[5].lower().split(\" \")\n","\t# remove stopwords\n","    source_abstract = [token for token in source_title if token not in stpwds]\n","    source_abstract = [stemmer.stem(token) for token in source_title]\n","    \n","    target_abstract = target_info[5].lower().split(\" \")\n","    target_abstract = [token for token in target_title if token not in stpwds]\n","    target_abstract = [stemmer.stem(token) for token in target_title]\n","    \n","    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n","    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n","    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n","    same_journal_test.append(id_journ)\n","    overlap_abstract_test.append(len(set(source_abstract).intersection(set(target_abstract))))\n","   \n","    counter += 1\n","    if counter % 10000 == True:\n","        print(counter, \"testing examples processsed\\r\")\n","\n","        \n","overlap_title_test_sqrt = np.sqrt(overlap_title_test)\n","temp_diff_test_sqrt = np.sqrt(np.abs(temp_diff_test))\n","overlap_abstract_test_sqrt = np.sqrt(overlap_abstract_test)\n","\n","# convert list of lists into array\n","# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n","testing_features = np.array([source_degree_centrality_journal, target_degree_centrality_journal, pref_attach_journal, aai_journal, jacard_coeff_journal, overlap_title_test,temp_diff_test,comm_auth_test, same_journal_test, overlap_abstract_test, overlap_title_test_sqrt, temp_diff_test_sqrt, overlap_abstract_test_sqrt, source_degree_centrality_journal, target_degree_centrality_journal, pref_attach_journal, aai_journal, jacard_coeff_journal, source_degree_centrality_author, target_degree_centrality_author, pref_attach_author, aai_author, jacard_coeff_author, source_degree_centrality_author_mean, target_degree_centrality_author_mean, pref_attach_author_mean, aai_author_mean, jacard_coeff_author_mean]).T"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-03-09T19:11:06.526813Z","iopub.status.busy":"2022-03-09T19:11:06.526529Z","iopub.status.idle":"2022-03-09T19:11:06.608944Z","shell.execute_reply":"2022-03-09T19:11:06.607964Z","shell.execute_reply.started":"2022-03-09T19:11:06.526781Z"},"trusted":true},"outputs":[],"source":["testing_features = poly.fit_transform(testing_features)\n","testing_features = scaler.transform(testing_features)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(32648, 300)\n"]}],"source":["print(testing_features.shape)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-03-09T19:11:06.619827Z","iopub.status.busy":"2022-03-09T19:11:06.619472Z","iopub.status.idle":"2022-03-09T19:11:07.434054Z","shell.execute_reply":"2022-03-09T19:11:07.432818Z","shell.execute_reply.started":"2022-03-09T19:11:06.619783Z"},"trusted":true},"outputs":[],"source":["# issue predictions\n","predictions_rand_for = list(classifier.predict(testing_features))\n","for i in range(len(predictions_rand_for)):\n","    if predictions_rand_for[i] <= 0.5:\n","        predictions_rand_for[i] = 0\n","    else:\n","        predictions_rand_for[i] = 1\n","# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n","predictions = zip(range(len(testing_set)), predictions_rand_for)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-03-09T19:11:07.436097Z","iopub.status.busy":"2022-03-09T19:11:07.435392Z","iopub.status.idle":"2022-03-09T19:11:07.476763Z","shell.execute_reply":"2022-03-09T19:11:07.475789Z","shell.execute_reply.started":"2022-03-09T19:11:07.436054Z"},"trusted":true},"outputs":[],"source":["name = \"improved_predictions_20000_300_features_svm.csv\"\n","\n","with open(name,\"w\") as pred1:\n","    csv_out = csv.writer(pred1)\n","    csv_out.writerow((\"id\",\"category\"))\n","    for row in predictions:\n","        csv_out.writerow(row)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
